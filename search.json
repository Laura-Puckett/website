[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Laura Puckett",
    "section": "",
    "text": "About\nI am an aspiring data scientist and ecologist. It all started when I joined the Ecosystem Dynamics and Forecasting Lab while getting my B.S. from Virginia Tech. I quickly realized that I really like writing scripts to analyze and visualize environmental datasets. Wanting to get more experience working with remote sensing and large spatial datasets, I joined the Global Earth Observation and Dynamics of Ecosystem Lab at Northern Arizona University and completed an M.S. in Informatics. Now I am looking for a new opportunity to put my data visualization and environmental modeling skills to use."
  },
  {
    "objectID": "portfolio/GEDI.html",
    "href": "portfolio/GEDI.html",
    "title": "Comparison of Satellite and Airborne LiDAR data",
    "section": "",
    "text": "Untitled\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this project, I created a workflow to compare GEDI and ICESat-2 against high-quality discrete-return LiDAR data for an area of interest.\n\n\nThe main steps are to:\n\n\n\ndownload and subset ICESat-2 data for an area of interest\n\n\ncreate spatial geometries from ICESat-2 and GEDI data csv files\n\n\nthis involved creating different geometries for the two instruments, because they have different pulse footprints\n\n\nextract corresponding discrete-return data for each geometry and calculate height metrics\n\n\nintersect geometries with forest group and forest cover layers and calculate summary values for each polygon\n\n\nplot the results\n\n\n\nCode for the project is available in a Github Repository\n\n\n\nPart 1. Download Data\n\n\nIn R, define the boundary.\n\nxmin=-111.5872356\nxmax=-111.538214\nymin=35.1363133\nymax=35.159778\n\nNow write to boundary to a shapefile. The resulting shapefile can be used to find overlapping files in the data portals.\n\nsource('./write_boundary_shp.R')\nwrite_boundary_shp(xmin, xmax, ymin, ymax, './boundary/boundary.shp')\n\n\nDiscrete-Return LiDAR Data\n\n\nDownload 3DEP USGS Discrete-Return LiDAR Data within the area of interest using the USGS LidarExplorer: https://prd-tnm.s3.amazonaws.com/LidarExplorer/index.html#\n\n\n\n\n\n\nGEDI\n\n\nDownload GEDI Level 2A Version 2 Data from https://search.earthdata.nasa.gov/\n\n\nI uploaded the shapefile of the boundary area and found 9 GEDI granules that intersect the area of interest. The download urls are shown below. Note, these files are large ~ 2 Gb each.  https://e4ftl01.cr.usgs.gov//GEDI_L1_L2/GEDI/GEDI02_A.002/2021.07.05/GEDI02_A_2021186140546_O14513_03_T03204_02_003_02_V002.h5 https://e4ftl01.cr.usgs.gov//GEDI_L1_L2/GEDI/GEDI02_A.002/2021.06.21/GEDI02_A_2021172123028_O14295_02_T06887_02_003_02_V002.h5 https://e4ftl01.cr.usgs.gov//GEDI_L1_L2/GEDI/GEDI02_A.002/2021.01.28/GEDI02_A_2021028044259_O12058_03_T06203_02_003_02_V002.h5 https://e4ftl01.cr.usgs.gov//GEDI_L1_L2/GEDI/GEDI02_A.002/2021.01.08/GEDI02_A_2021008123045_O11753_03_T08896_02_003_02_V002.h5 https://e4ftl01.cr.usgs.gov//GEDI_L1_L2/GEDI/GEDI02_A.002/2020.06.03/GEDI02_A_2020155031743_O08352_03_T00511_02_003_01_V002.h5 https://e4ftl01.cr.usgs.gov//GEDI_L1_L2/GEDI/GEDI02_A.002/2019.11.26/GEDI02_A_2019330230004_O05419_02_T05464_02_003_01_V002.h5 https://e4ftl01.cr.usgs.gov//GEDI_L1_L2/GEDI/GEDI02_A.002/2019.10.31/GEDI02_A_2019304093108_O05007_02_T04041_02_003_01_V002.h5 https://e4ftl01.cr.usgs.gov//GEDI_L1_L2/GEDI/GEDI02_A.002/2019.07.09/GEDI02_A_2019190134530_O03241_03_T04780_02_003_01_V002.h5 https://e4ftl01.cr.usgs.gov//GEDI_L1_L2/GEDI/GEDI02_A.002/2019.04.23/GEDI02_A_2019113131121_O02045_02_T01195_02_003_01_V002.h5\n\n\n\nICESat-2\n\n\nDownload ICESat-2 L3A Land and Vegetaion Height, Version 5 data using the python download script, nsidc-download_ATL08.005_2021-12-01.py, that is automatically generated at https://nsidc.org/data/atl08  In a terminal, run the following:\n\npy ./nsidc-download_ATL08.005_2021-12-01.py\n\nPart 2. Spatially Subset the Data\n\n\n\nSpatially subset the GEDI data by cloning the GEDI-subsetter by Cole Krehbiel from https://git.earthdata.nasa.gov/projects/LPDUR/repos/gedi-subsetter/browse/GEDI_Subsetter.py\n\n In a terminal, run the following:\n\nconda create -n gedi -c conda-forge --yes python=3.7 h5py shapely geopandas pandas\nconda activate gedi\npython ./gedi-subsetter/GEDI_Subsetter.py --dir ./data/0_orig/gedi/ --roi 35.159778,-111.5872356,35.1363133,-111.538214 \n\n** Note: From this point on, all code is written for R.**\n\n\nSpatially subset the ICESat-2 data and convert from hdf to csv.\n\nsource('./hdf_to_csv.R')\nhdf_to_csv('./data/0_orig/icesat2/','./data/1_subset/icesat2/icesat2.csv', ymax, xmin, ymin, xmax)\n\nPart 3. Create Spatial Geometries\n\n\nConvert the given centroid coordinates for the ICESat-2 and GEDI footprint to spatial geometries . This process results in 100m x 14m rectangles for the ICESat-2 canopy data and 25m dimater circles for the GEDI data.\n\n# define a projected coordinate system to use for spatial manipulation\nprojection = sp::CRS(\"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\")\n\nsource('./icesat2_csv_to_geometries.R')\ncsv_to_geometries(csv_path = './data/1_subset/icesat2/icesat2.csv',\n                  geometry_path = './data/1_subset/icesat2/icesat2.shp',\n                  proj = projection)\n\nsource('./gedi_json_to_geometries.R')\njson_to_geometries(json_path = './data/1_subset/gedi/',\n                   './data/1_subset/gedi/gedi.shp',\n                   proj = projection)\n\nNow let’s visualize the spatial relationship of the datasets.\n\n# load the saved shapefiles\nboundary = rgdal::readOGR('./boundary/boundary.shp', verbose = F)\nicesat_geoms = rgdal::readOGR('./data/1_subset/icesat2/icesat2.shp', verbose = F)\ngedi_geoms = rgdal::readOGR('./data/1_subset/gedi/gedi.shp', verbose = F)\n\nlibrary(leaflet);\n\n# produce the map\nleaflet() %&gt;%\n  setView(lng = (xmin + xmax)/2, lat = (ymin + ymax)/2, zoom = 14) %&gt;%\n  addProviderTiles(providers$Esri.WorldImagery, options = providerTileOptions(opacity = 0.5)) %&gt;%\n  addPolygons(data = icesat_geoms, fillColor = \"blue\", fillOpacity = 0.3, stroke = FALSE) %&gt;%\n  addPolygons(data = gedi_geoms, fillColor = \"red\", fillOpacity = 0.5, stroke = FALSE) %&gt;%\n  addPolygons(data = boundary, color = \"black\", fillOpacity = 0) %&gt;%\n  addLegend(colors = c(\"blue\",\"red\",\"black\"), labels = c(\"ICESat-2 footprints\", \"GEDI footprints\",\"3DEP coverage\"), opacity = 1)\n\n\n\n\n\nPart 4. Extract Discrete-Return Values for Satellite Footprints\n\n\nIn the following step, a function is used to calculate relative height metrics from the intersecting part of the discrete-return point cloud for each satellite footprint and output the resulting data as a csv.\n\nextract_ALS_metrics(sensor = \"icesat2\",\n                    geom_path = \"./data/1_subset/icesat2/icesat2.shp\",\n                    output_path = \"./data/2_joined/icesat_ALS.csv\",\n                    ALS_dir = './data/0_orig/USGS/',\n                    ctg_path = './data/ctg.RDS',\n                    proj = projection)\n\nextract_ALS_metrics(sensor = \"gedi\",\n                    geom_path = \"./data/1_subset/gedi/gedi.shp\",\n                    output_path = \"./data/2_joined/gedi_ALS.csv\",\n                    ALS_dir = './data/0_orig/USGS/',\n                    ctg_path = './data/ctg.RDS',\n                    proj = projection)\n\nBelow are visualizations of clipping the discrete-return point cloud to satellite sensor footprints. For each instruments, the point cloud is clipped to the shape of the footprint (100x14m rectangle for ICESat-2, 25m diameter circle for GEDI) The relative height metrics associated with each footprint are saved, and corresponding relative height metrics for the discrete-return data are calculated from the clipped point cloud.\n\n\nDiscrete-Return Point Cloud Clipped to an ICESat-2 Footprint\n\n\n\n\n\n\nDiscrete-Return Point Cloud Clipped to a GEDI Footprint\n\n\n\n\n\n\nPart 5. Compare the Datasets\n\nlibrary(dplyr, quietly = T)\n# Prepare ICESat-2 Data\nicesat = read.csv('./data/2_joined/icesat_ALS.csv') %&gt;%\n  dplyr::select(cn_h_25, cn_h_50, cn_h_75, cn_h_95,cn_max,\n                RH_25_ALS, RH_50_ALS,RH_75_ALS, RH_95_ALS, RH_98_ALS, RH_100_ALS) %&gt;%\n  mutate(sensor = \"ICESat-2\") %&gt;%   # remove No Data values\n  filter(cn_max &lt; 3.402823e+23) %&gt;%\n  rename(RH_25_sat = cn_h_25,\n         RH_50_sat = cn_h_50,\n         RH_75_sat = cn_h_75,\n         RH_95_sat = cn_h_95,\n         RH_100_sat = cn_max)\n# Prepare GEDI Data\ngedi = read.csv('./data/2_joined/gedi_ALS.csv') %&gt;%\n  dplyr::select(rh_25, rh_50, rh_75, rh_95, rh_100,\n                RH_25_ALS, RH_50_ALS,RH_75_ALS, RH_95_ALS, RH_98_ALS, RH_100_ALS) %&gt;%\n  mutate(sensor = \"GEDI\") %&gt;%\n  rename(RH_25_sat = rh_25,\n         RH_50_sat = rh_50,\n         RH_75_sat = rh_75,\n         RH_95_sat = rh_95,\n         RH_100_sat = rh_100)\n\n# combine the datasets\ncombined = rbind(icesat,gedi)\nlibrary(plotly, quietly = T); \nplot_ly() %&gt;%\n  layout(title = \"Comparison of Canopy Height Values [m] \",\n         yaxis = list(title='Satellite Data',\n                      dtick = 10),\n         xaxis = list(title = \"3DEP Discrete-Return Data\",\n                      dtick = 10)) %&gt;%\n  add_trace(data = combined,\n            name = ~sensor,\n            type = \"scatter\",\n            color = ~sensor,\n            alpha = 0.5,\n            x = ~RH_98_ALS,\n            y = ~RH_100_sat) %&gt;%\n  add_trace(x = c(0:45),\n            y = c(0:45),\n            name = \"1:1 line\",\n            color = \"red\",\n            mode = \"lines\")\n\n\n\n\nlibrary(tidyr, quietly = T)\n# reform data to have a column for RH percentile and a column for the height values\nby_RH = combined %&gt;%\n  dplyr::select(-RH_98_ALS) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  gather(key = \"RH_num_type\", value = \"value\", -sensor, -id) %&gt;%\n  # pull out the numeric values\n  mutate(RH_num = substr(RH_num_type, 4, nchar(RH_num_type)-4)) %&gt;%\n  # pull out \"sat\" or \"ALS\"\n  mutate(type = substr(RH_num_type, nchar(RH_num_type)-2, nchar(RH_num_type)),\n         # change \"sat\" to either ICESat-2 or GEDI\n         type = ifelse(type == \"sat\",sensor, type),\n         # change ALS to be more descriptive for figure legend\n         type = ifelse(type == \"ALS\", \"3DEP Discrete-Return\", type))\n\n# reorder the factors so it doesn't start with 100\nby_RH$RH_num = factor(by_RH$RH_num, levels = c(\"25\",\"50\",\"75\",\"95\",\"100\"))\nfig1 = plot_ly() %&gt;%\n  layout(title = \"\",\n         yaxis=list(title='Height [m]'),\n         xaxis = list(title = \"Relative Height Percentile\"),\n         legend = list(x = 0.05, y = 0.95)) %&gt;%\n  add_boxplot(data = by_RH %&gt;% filter(sensor == \"GEDI\"),\n              name = ~type,\n              legendgroup = 'a',\n              x = ~RH_num,\n              y = ~value,\n              color = ~type) %&gt;%\n  layout(boxmode = \"group\")\n\nfig2 = plot_ly() %&gt;%\n  layout(title = \"\",\n         yaxis=list(title='Height [m]'),\n         xaxis = list(title = \"Relative Height Percentile\")) %&gt;%\n  add_boxplot(data = by_RH %&gt;% filter(sensor == \"ICESat-2\"),\n              name = ~type,\n              legendgroup = 'b',\n              x = ~RH_num,\n              y = ~value,\n              color = ~type,\n              colors = \"Dark2\") %&gt;%\n  layout(boxmode = \"group\")\n\nfig &lt;- subplot(fig1, fig2, nrows = 2, titleX = T, titleY = T, shareX = T, shareY = T) %&gt;%\n  layout(title = list(text = \"Height Values Comparison for Multiple Relative Height Percecntiles \"),\n         yaxis=list(title='Height [m]'),\n         xaxis = list(title = \"Relative Height Percentile\"))\nfig\n\n\n\n\n\n\nOverall, there is a clear relationship between values from the satellite instruments and high-quality 3DEP discrete-return LiDAR data. However, there are some outliers and more could be done to explore these relationships. Filtering for nighttime data, GEDI strong beams, and thresholds for the quality flags would yield the highest quality data for filtering. However, that may not leave us with much data in the small area used for this example.\n\n\n\n\n\nHere is an example where those filters were used in an analysis done in 2020. The data is from a larger area near Lake Tahoe. The code for this project was slightly different because the project was set up to run on a high-performance computing cluster due to the large size of the datasets. That code is available on the “NAU” fork of the same repository.\n\n\n\n\n\n\nDataset References\n\n\n\nDubayah, R., Tang, H., Armston, J., Luthcke, S., Hofton, M., Blair, J. (2020). GEDI L2B Canopy Cover and Vertical Profile Metrics Data Global Footprint Level V001 [Data set]. NASA EOSDIS Land Processes DAAC. Accessed 2021-11-30 from https://doi.org/10.5067/GEDI/GEDI02_B.001 \n\n\nNeuenschwander, A. L., K. L. Pitts, B. P. Jelley, J. Robbins, B. Klotz, S. C. Popescu, R. F. Nelson, D. Harding, D. Pederson, and R. Sheridan. 2021. ATLAS/ICESat-2 L3A Land and Vegetation Height, Version 5. [Indicate subset used]. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. doi: https://doi.org/10.5067/ATLAS/ATL08.005. [2020-12-01]. \n\n\nAZ Coconino 2019 B19: Discrete-Return LiDAR Data downloaded from USGS 3DEP LiDAR Explorer [2020-11-30].\n\n\n\n\nAppendix\n\n\nwrite_boundary_shp function\n\nwrite_boundary_shp = function(xmin, xmax, ymin, ymax, filepath){\n  # Author: Laura Puckett.\n  # 11/30/2021\n  # Purpose: Write a shapefile from bounding box coordinates in WGS84.\n  library(sp); library(rgdal);\n\n  coords = c(c(xmin, xmin, xmax, xmax),c(ymin, ymax, ymax, ymin))\n\n  Srl = NULL\n  x = c(xmax, xmax, xmin, xmin, xmax)\n  y = c(ymin, ymax, ymax, ymin, ymin)\n  poly = Polygon(coords = cbind(x, y))\n  Polygons_obj = Polygons(srl = c(poly), ID = as.numeric(1))\n  Srl = append(Polygons_obj, Srl)\n\n  Sr = SpatialPolygons(Srl = Srl, proj4string = sp::CRS(\"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0\"))\n  spdf = SpatialPolygonsDataFrame(Sr = Sr, data = as.data.frame(NA), match.ID = T)\n  spdf@data = data.frame(\"X\" = 1)\n  writeOGR(obj = spdf,\n           dsn = filepath,\n           layer = 'boundary',\n           overwrite_layer = T,\n           driver = 'ESRI Shapefile')\n}\n\nhdf_to_csv Function\n\nhdf_to_csv = function(icesat_dir, output_path, ymax, xmin, ymin, xmax){\n\n  # *********************************************************************************#\n  # -----0. Packages\n  # ********************************************************************************#\n  if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n  if (!requireNamespace(\"rhdf5\", quietly = TRUE))\n    BiocManager::install(\"rhdf5\")\n  library(rhdf5)\n\n  # *********************************************************************************#\n  # -----1: Body\n  # ********************************************************************************#\n  icesat_files = list.files(icesat_dir, pattern = \"h5\", recursive = TRUE)\n  # print(c(\"icesat files: \", icesat_files))\n\n  df = NULL\n  count=0\n  for(beam in c(\"1l\",\"1r\",\"2l\",\"2r\",\"3l\",\"3r\")){\n    for (filename in icesat_files){\n      tmp = try(rhdf5::h5read(file = paste0(icesat_dir, filename), paste0(\"/gt\", beam, \"/\")), silent = TRUE)\n      # in some .h5 files, there isn't info for every beam\n      if(class(tmp) == \"try-error\"){\n        print(paste0(\"Information from file \", filename, \"is missing for beam \", beam))\n      }else if(is.null(tmp$land_segments)==FALSE){\n        df.tmp = NULL\n\n        # extract metadata\n        df.tmp = cbind(df.tmp, \"seg_id_beg\" = tmp$land_segments$segment_id_beg)\n        df.tmp = cbind(df.tmp, \"seg_id_end\" = tmp$land_segments$segment_id_end)\n        df.tmp = cbind(df.tmp, \"lat\" = as.numeric(tmp$land_segments$latitude))\n        df.tmp = cbind(df.tmp, \"lon\" = as.numeric(tmp$land_segments$longitude))\n        df.tmp = cbind(df.tmp, \"h_te_mean\" = as.numeric(tmp$land_segments$h_te_mean))\n        df.tmp = cbind(df.tmp, \"n_te_photons\" = as.numeric(tmp$land_segments$n_te_photons))\n        df.tmp = cbind(df.tmp, \"h_te_uncertainty\" = as.numeric(tmp$land_segments$h_te_uncertainty))\n        df.tmp = cbind(df.tmp, \"terrain_slope\" = as.numeric(tmp$land_segments$terrain_slope))\n        df.tmp = cbind(df.tmp, \"night_flag\" = tmp$land_segments$night_flag)\n        df.tmp = cbind(df.tmp, \"landcover\" = tmp$land_segments$segment_landcover)\n\n        # extract canopy related values\n        df.tmp = cbind(df.tmp, \"cn_mean\" = tmp$land_segments$canopy$h_mean_canopy)\n        df.tmp = cbind(df.tmp, \"cn_max\"  = tmp$land_segments$canopy$h_max_canopy)\n        df.tmp = cbind(df.tmp, \"h_can\"  = tmp$land_segments$canopy$h_canopy)\n        df.tmp = cbind(df.tmp, \"cn_med\"  = tmp$land_segments$canopy$h_median_canopy)\n        df.tmp = cbind(df.tmp, \"cn_h_25\" = unlist(tmp$land_segments$canopy$canopy_h_metrics)[1,])\n        df.tmp = cbind(df.tmp, \"cn_h_50\" = unlist(tmp$land_segments$canopy$canopy_h_metrics)[2,])\n        df.tmp = cbind(df.tmp, \"cn_h_60\" = unlist(tmp$land_segments$canopy$canopy_h_metrics)[3,])\n        df.tmp = cbind(df.tmp, \"cn_h_70\" = unlist(tmp$land_segments$canopy$canopy_h_metrics)[4,])\n        df.tmp = cbind(df.tmp, \"cn_h_75\" = unlist(tmp$land_segments$canopy$canopy_h_metrics)[5,])\n        df.tmp = cbind(df.tmp, \"cn_h_80\" = unlist(tmp$land_segments$canopy$canopy_h_metrics)[6,])\n        df.tmp = cbind(df.tmp, \"cn_h_85\" = unlist(tmp$land_segments$canopy$canopy_h_metrics)[7,])\n        df.tmp = cbind(df.tmp, \"cn_h_90\" = unlist(tmp$land_segments$canopy$canopy_h_metrics)[8,])\n        df.tmp = cbind(df.tmp, \"cn_h_95\" = unlist(tmp$land_segments$canopy$canopy_h_metrics)[9,])\n        df.tmp = cbind(df.tmp, \"cn_open\" = tmp$land_segments$canopy$canopy_openness)\n        df.tmp = cbind(df.tmp, \"toc_rough\" = tmp$land_segments$canopy$toc_roughness)\n        df.tmp = cbind(df.tmp, \"cn_unc\" = tmp$land_segments$canopy$h_canopy_uncertainty)\n        df.tmp = cbind(df.tmp, \"cn_pho\" = tmp$land_segments$canopy$n_ca_photons)\n        df.tmp = cbind(df.tmp, \"toc_pho\" = tmp$land_segments$canopy$n_toc_photons)\n        df.tmp = cbind(df.tmp, \"cn_close\" = tmp$land_segments$canopy$canopy_closure)\n        df.tmp = cbind(df.tmp, \"n_pho\" = tmp$land_segments$canopy$n_photons)\n\n        # add up canopy flags across the 5 sections of the canopy segment\n        sub_cn_flag_sum = unlist(tmp$land_segments$canopy$subset_can_flag)[1,] +\n          unlist(tmp$land_segments$canopy$subset_can_flag)[2,] +\n          unlist(tmp$land_segments$canopy$subset_can_flag)[3,]+\n          unlist(tmp$land_segments$canopy$subset_can_flag)[4,] +\n          unlist(tmp$land_segments$canopy$subset_can_flag)[5,]\n        df.tmp = cbind(df.tmp, \"sub_cn_flag_sum\" = sub_cn_flag_sum)\n        df.tmp = cbind(df.tmp, \"ls_flag\" = tmp$land_segments$canopy$landsat_flag)\n\n        # Add Information Included in the Filename\n        df.tmp = as.data.frame(df.tmp)\n        df.tmp = cbind(df.tmp, \"beam\" = beam)\n        df.tmp = cbind(df.tmp, \"filename\" = filename)\n        df.tmp = cbind(df.tmp, \"year\" = substr(filename, 7, 10))\n        df.tmp = cbind(df.tmp, \"month\" = substr(filename, 11, 12))\n        df.tmp = cbind(df.tmp, \"day\" = substr(filename, 13, 14))\n        df.tmp = cbind(df.tmp, \"hhmmss\" = substr(filename, 15,20))\n        df = rbind(df, df.tmp)\n      }\n    }\n  }\n\n  # Spatially subset the data\n  print(nrow(df))\n  print(\"filtering for bounding box\")\n  df = df[which(df$lat&gt;ymin & df$lat&lt;ymax & df$lon&gt;xmin & df$lon&lt;xmax),]\n\n  write.csv(df, output_path)\n}\n\ncsv_to_geometries function\n\ncsv_to_geometries = function(csv_path, geometry_path, proj){\n  # Author: Laura Puckett\n  # 08/26/2020\n  # Purpose: Convert a csv file of icesat-2 data into a shapefile of spatial geomtries representing the footprint of the canopy-level data, which is aggregated every 100m along-track.\n\n  print(\"---Arguments---\")\n  print(paste0(\"icesat path: \", csv_path))\n  print(paste0(\"geometry_path: \", geometry_path))\n  print(paste0(\"proj: \", proj))\n\n  ####------------------------------------------------------------------------####\n  #### Create footprint geometries\n  ####------------------------------------------------------------------------####\n\n  # https://nsidc.org/sites/nsidc.org/files/technical-references/ICESat2_ATL08_data_dict_v003.pdf\n  # states that lat and long are expressed as the center-most photon, aggregated in 100m chunks\n\n  # https://nsidc.org/sites/nsidc.org/files/technical-references/ICESat2_ATL08_ATBD_r003.pdf\n  # states that the diameter for the 100m aggregated tracks is about 14m\n\n  icesat_data = read.csv(csv_path, stringsAsFactors = FALSE)\n\n  coord &lt;- cbind(as.numeric(icesat_data$lon), as.numeric(icesat_data$lat))\n  icesat_spdf = sp::SpatialPointsDataFrame(coord, icesat_data)\n  rm(coord)\n  sp::proj4string(icesat_spdf) = sp::CRS(\"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0\")\n  icesat = spTransform(icesat_spdf, proj)\n  rm(icesat_data)\n\n  footprint_spdf = NULL\n  print(paste0(\"total number of points: \", nrow(icesat@data)))\n\n  for(footprintNum in 1:nrow(icesat@data)){\n    print(footprintNum)\n\n    ####------------------------------------------------------------------------####\n    #### get data from one icesat-2 point for a single beam\n    ####------------------------------------------------------------------------####\n    point = sp::SpatialPointsDataFrame(\n      coords = cbind(icesat@coords[footprintNum,1], icesat@coords[footprintNum,2]),\n      proj4string = proj,\n      data = icesat@data[footprintNum,])\n\n    ####------------------------------------------------------------------------####\n    #### make the geometry for the rectangular footprint of the point\n    ####------------------------------------------------------------------------####\n    # the idea here is to:\n    # (a) use a pair of points to figure out what direction the \"track\" is,\n    # (b) use that to construct a line,\n    # (c) extend the line to the other side of the point of interest,\n    # (d) grab a portion of that line that has 50m on either side of the point of interest,\n    # (e) buffer that line by the appropriate width to get the rectangle that\n    #  the \"point\" of aggregated data represents\n\n    sorted = icesat@data %&gt;% # finding the previous point on the line segment\n      filter(filename == point@data$filename & beam == point@data$beam) %&gt;%\n      arrange(seg_id_beg)\n\n    point_segid_index = which(sorted$seg_id_beg == point@data$seg_id_beg)\n\n    if(point_segid_index &gt; 1){\n      prev_seg_id = sorted[point_segid_index-1,\"seg_id_beg\"]\n      prev_point_num = which(icesat@data$seg_id_beg == prev_seg_id & icesat@data$filename == point@data$filename & icesat@data$beam == point@data$beam)\n      prev_point_coords = cbind(x1 = icesat@coords[prev_point_num,1],x2 = icesat@coords[prev_point_num,2])\n\n      # comparing points from two points along the line\n      x1 = point@coords[1]\n      y1 = point@coords[2]\n      x0 = prev_point_coords[1]\n      y0 = prev_point_coords[2]\n      xdiff = x1-x0\n      ydiff = y1-y0\n\n      # extending that line to both sides of x0,y0\n      xy &lt;- cbind(c(x1-xdiff, x1+xdiff),c(y1-ydiff, y1+ydiff))\n      xy.sp = sp::SpatialPoints(xy)\n      line &lt;- SpatialLines(list(Lines(Line(xy.sp), ID=footprintNum)))\n      line@proj4string = proj\n\n      # intersect that with pointBuffer to retain only the portion of the line that\n      # will be used to create the footprint box\n      pointBuffer = gBuffer(point, width = 50, byid = TRUE)\n      intsct = gIntersection(line, pointBuffer) # get 50m of line segment on either side of point\n\n      # buffer that line to the appropriate width (14m across)\n      footprint = gBuffer(intsct, width=(14/2), capStyle = \"FLAT\")\n      footprint = sp::spTransform(footprint, CRSobj = crs(\"+init=epsg:4326\"))\n\n\n      ####------------------------------------------------------------------------####\n      #### ----- Save the data with the geometries\n      ####------------------------------------------------------------------------####\n      footprint_spdf_tmp = SpatialPolygonsDataFrame(footprint[1],  point@data, match.ID = F)\n\n      if(is.null(footprint_spdf)){\n        footprint_spdf = footprint_spdf_tmp\n      }else{\n        footprint_spdf = rbind(footprint_spdf, footprint_spdf_tmp)\n      }\n    }\n  }\n  writeOGR(obj = footprint_spdf, dsn = geometry_path, layer = \"icesat2\", driver = \"ESRI Shapefile\", overwrite_layer = T)\n} \n\njson_to_geometries function\n\njson_to_geometries = function(json_path, geometry_path, proj){\n  # Author: Laura Puckett\n  # 11/30/2020\n  # Purpose: Convert json files of subsetted GEDI footprints to a single shapefile of polygons representing the GEDI footprints\n  points = NULL\n  for(file in list.files(json_path, pattern = \".json\")){\n    points_tmp = readOGR(paste0(json_path, file))\n\n    if(is.null(points)){\n      points = points_tmp\n    }else{\n      points = rbind(points, points_tmp)\n    }\n  }\n\n  # transform to projected coordinate system for buffering\n  points = spTransform(points, proj)\n  # create 12.5m radius geometries from gedi footprint center points\n  geoms = rgeos::gBuffer(points, width = 12.5, byid = T) %&gt;%\n    # convert back to WGS84\n    spTransform(sp::CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\"))\n  writeOGR(geoms, layer = \"geometry\", geometry_path, driver = \"ESRI Shapefile\", overwrite_layer = T)\n}\n\nextract_ALS_metrics function\n\n#  Define function to calculate height metrics for aerial LiDAR Survey (ALS) data within geometries\nextract_ALS_metrics = function(sensor, geom_path, output_path, ALS_dir, ctg_path, proj){\n  # Author: Laura Puckett\n  # 08/26/2020\n  # Purpose: extract discrete-return Aerial LiDAR Survey (ALS) data within a given polygon, calculate relative height metrics, and output results as a csv file.\n\n  list.of.packages &lt;- c(\"lidR\",\"rgdal\", \"raster\",\"dplyr\", \"sp\")\n\n  new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n  if(length(new.packages)&gt;0) install.packages(new.packages, repos='http://cloud.r-project.org')\n\n  library(lidR); library(rgdal); library(raster); library(dplyr); library(sp)\n\n  if(file.exists(ctg_path)){\n    ctg = readRDS(ctg_path)\n  }else{\n    ctg = readLAScatalog(ALS_dir)\n    saveRDS(object = ctg, file = ctg_path)\n  }\n\n  geoms_WGS84 = rgdal::readOGR(geom_path)\n  geoms = spTransform(geoms_WGS84, proj)\n  data = NULL\n\n  # clip point cloud to a buffered version of the geometry (keeping edges for ground surface interpolation)\n  for(i in 1:nrow(geoms@data)){\n    print(i)\n    geom = geoms[i,]\n\n    las_bufferclip = lidR::clip_roi(las = ctg, geometry = rgeos::gBuffer(geom, 5, byid=T))\n    if(nrow(las_bufferclip@data)&gt;0){\n\n      # normalize point cloud to the ground\n      norm_las = normalize_height(las_bufferclip, knnidw())\n\n      ## Summarize point cloud over entire geometry\n      norm_las_clip = clip_roi(norm_las, geom)\n      if(length(norm_las_clip@data$NumberOfReturns)&gt;0){\n        # filter for unclassified or vegetation points\n        veg = filter_poi(norm_las_clip, Classification %in% c(1L, 3L, 4L, 5L))\n        # calculate metrics defined in myMetrics function\n        las_met = cloud_metrics(las = norm_las_clip,  func = myMetrics(Z))\n        geom_data = cbind(geom@data, as.data.frame(las_met))\n\n        # can't rbind to NULL, so using this to initialize the dataframe\n        if(is.null(data)){\n          data = geom_data\n        }else{\n          data = rbind(data, geom_data)\n        }\n      }\n    }\n  }\n  write.csv(data, output_path)\n}\n\nmyMetrics = function(z) {\n  metrics = list(\n    RH_100_ALS = quantile(z, probs = c(1.00)),\n    RH_99_ALS = quantile(z, probs = c(0.99)),\n    RH_98_ALS = quantile(z, probs = c(0.98)),\n    RH_95_ALS = quantile(z, probs = c(0.95)),\n    RH_75_ALS = quantile(z, probs = c(0.75)),\n    RH_50_ALS = quantile(z, probs = c(0.50)),\n    RH_25_ALS = quantile(z, probs = c(0.25)))\n  return(metrics)\n}"
  },
  {
    "objectID": "portfolio/weather.html",
    "href": "portfolio/weather.html",
    "title": "Statistical Downscaling of Meteorological Forecasts",
    "section": "",
    "text": "In this project, I temporally and spatially downscaled forecasts for five different meteorological metrics for use in a reservoir hydrodynamics forecasting project. I temporally downscaled the 21 climate model ensemble member forecasts from 6-hr to hourly resolution and spatially downscaled to our specific site, the Falling Creek Reservoir in Vinton, VA. Standard error in the statistical model fits was recorded and used to generate ensemble members with random noise proportional to model error. This allowed meteorological uncertainty to be propagated through further steps in the hydrodynamic forecasting project.\nThis work contributed to the publication, A Near-Term Iterative Forecasting System Successfully Predicts Reservoir Hydrodynamics and Partitions Uncertainty in Real Time.\n\nWritten Report link\nCode Repository link\n\n\n\n\n\n\n\n\n\nObserved and 1-day Downscaled Forecasts over a 14-day fall period"
  },
  {
    "objectID": "portfolio/NEON.html",
    "href": "portfolio/NEON.html",
    "title": "NEON Woody Vegetation Visualization",
    "section": "",
    "text": "Laura Puckett\n\n\n11/29/2021\n\n\n\nA simple project to download, organize, and interactively view NEON data. This was a fun project to get practice accesing data through APIs and building interactive apps.\n\n\n\nInteractive App link\n\n\nCode Repository link\n\n\n\nMy code for this project is shown in a reproducible example below.\n\n\nPart 1. Download Data\n\n\nLoad Libraries\n\n\n\nR version: 4.1.2\n\nlibrary(neonUtilities);  library(ggplot2)\n library(dplyr); library(shinyr); library(shinythemes)\n\nDownload Data using NEON API\n\n\nlibrary(neonUtilities);\n\n# load my neon token as an object called NEON_TOKEN\nsource('./neon_token.R')\n\n# instructions for getting a NEON API token can be found here: https://www.neonscience.org/resources/learning-hub/tutorials/neon-api-tokens-tutorial\n\nveglist = loadByProduct(dpID=\"DP1.10098.001\",\n                        site = \"all\",\n                        package = \"basic\",\n                        check.size = F,\n                        token = NEON_TOKEN)\nsaveRDS(veglist, './veglist.Rdata')\n\nPart 2. Organize Data\n\n\nLoad datasets of interest and select columns of interest\n\n\nveglist = readRDS('./veglist.Rdata')\n\nveg_ind = veglist$vst_apparentindividual %&gt;%\n  dplyr::select(siteID, plotID, height, individualID, plantStatus,\n                stemDiameter, date, eventID)\n\nveg_loc = veglist$vst_mappingandtagging %&gt;%\n  dplyr::select(domainID, siteID, plotID, individualID, taxonID, scientificName) %&gt;%\n  unique()\n\nplot_info = veglist$vst_perplotperyear %&gt;%\n  select(eventID, siteID, plotID, plotType) %&gt;%\n  unique()\n\nrm(veglist) # unload the large file now that it's no longer needed\n\nJoin Datasets\n\n\nveg = veg_ind %&gt;%\n  inner_join(veg_loc, by = c(\"individualID\", \"siteID\", \"plotID\")) %&gt;%\n  inner_join(plot_info, by = c(\"siteID\", \"plotID\", \"eventID\"))\n\nveg %&gt;% head()\n##   siteID   plotID height            individualID           plantStatus\n## 1   BART BART_044    5.6 NEON.PLA.D01.BART.05332 Live, disease damaged\n## 2   BART BART_037   11.5 NEON.PLA.D01.BART.05273 Live, disease damaged\n## 3   BART BART_037    1.8 NEON.PLA.D01.BART.05262 Live, disease damaged\n## 4   BART BART_044    0.8 NEON.PLA.D01.BART.05424                  Live\n## 5   BART BART_044   10.4 NEON.PLA.D01.BART.05415 Live, disease damaged\n## 6   BART BART_044    1.1 NEON.PLA.D01.BART.05319                  Live\n##   stemDiameter       date       eventID domainID taxonID\n## 1          5.8 2015-08-26 vst_BART_2015      D01    FAGR\n## 2         10.3 2015-08-26 vst_BART_2015      D01    FAGR\n## 3          1.6 2015-08-26 vst_BART_2015      D01    FAGR\n## 4          1.4 2015-08-26 vst_BART_2015      D01    TSCA\n## 5         12.5 2015-08-26 vst_BART_2015      D01    FAGR\n## 6          1.2 2015-08-26 vst_BART_2015      D01   PICEA\n##                   scientificName plotType\n## 1        Fagus grandifolia Ehrh.    tower\n## 2        Fagus grandifolia Ehrh.    tower\n## 3        Fagus grandifolia Ehrh.    tower\n## 4 Tsuga canadensis (L.) Carrière    tower\n## 5        Fagus grandifolia Ehrh.    tower\n## 6                      Picea sp.    tower\n\nFilter Data\n\n\nveg = veg %&gt;%\n  # filter for Live trees\n  filter(substr(plantStatus, 1,4) == \"Live\") %&gt;%\n  # filter for tree measurements (which start at 10cm)\n  filter(stemDiameter &gt; 10) %&gt;%\n  filter(is.na(height) == F & is.na(stemDiameter) == F) %&gt;%\n  # remove outliers\n  group_by(taxonID) %&gt;%\n  mutate(height_lower =  boxplot(height, plot=FALSE)$stats[1],\n         height_upper = boxplot(height, plot=FALSE)$stats[5],\n         diam_lower =  boxplot(stemDiameter, plot=FALSE)$stats[1],\n         diam_upper = boxplot(stemDiameter, plot=FALSE)$stats[5]) %&gt;%\n  filter(height &gt; height_lower & height &lt; height_upper,\n         stemDiameter &gt; diam_lower & stemDiameter &lt; diam_upper) %&gt;%\n  dplyr::select(-height_lower, -height_upper, -diam_lower, -diam_upper) %&gt;%\n  ungroup()\n\n# Select columns\nveg = veg %&gt;%\n  dplyr::select(siteID, stemDiameter, height, plantStatus, scientificName, taxonID, date, individualID)\n\nGet the 20 Most Commmon Species\n\n\n# Find top 20 species by observation count\nspecies = veg %&gt;%\n  group_by(taxonID, scientificName) %&gt;%\n  summarize(obs_count = n()) %&gt;%\n  arrange(desc(obs_count)) %&gt;%\n  head(20) %&gt;%\n  select(taxonID, scientificName) %&gt;%\n  ungroup() %&gt;%\n  mutate(num = row_number()) %&gt;%\n  as.data.frame()\n\nWrite Output Files for Shiny App\n\n\nsaveRDS(veg, './veg_data_for_shiny.Rdata')\nsaveRDS(species, './species.Rdata')\n\nPart 3. The Shiny app\n\n\nServer Function\n\n\n\nserver = function(input, output) {\n  selection = reactive({\n    as.numeric(input$optionNum)\n  })\n  selected_data &lt;- reactive({\n    df &lt;- veg %&gt;%\n      filter(taxonID == species$taxonID[as.numeric(input$optionNum)]) %&gt;%\n      dplyr::select(individualID, siteID,scientificName, plantStatus, date, stemDiameter, height)\n\n    if(!input$Live){\n      df = df %&gt;% filter(plantStatus != \"Live\")\n    }\n    if(!input$Live_DD){\n      df = df %&gt;% filter(plantStatus != \"Live, disease damaged\")\n    }\n    if(!input$Live_PD){\n      df = df %&gt;% filter(plantStatus != \"Live, physically damaged\")\n    }\n    if(!input$Live_BB){\n      df = df %&gt;% filter(plantStatus != \"Live, broken bole\")\n    }\n    if(!input$Live_ID){\n      df = df %&gt;% filter(plantStatus != \"Live, insect damaged\")\n    }\n    if(!input$Live_OD){\n      df = df %&gt;% filter(plantStatus != \"Live,  other damage\")\n    }\n\n    print(df)\n    df\n  })\n\n\n  output$table &lt;- DT::renderDataTable({\n    table &lt;- selected_data()\n  })\n\n  output$plot1 &lt;- renderPlot({\n    ggplot(data = selected_data(),\n           aes(x = stemDiameter, y = height)) +\n      theme_bw() +\n      geom_point(aes(color = siteID), alpha = 0.4) +\n      geom_smooth(color = \"black\",size = 0.5, se = F) +\n      xlab(\"Stem Diameter [cm]\") +\n      ylab(\"Tree Height [m]\")  +\n      # ggtitle(paste(\"Selected Species:\",\n      #               species$scientificName[selection()])) +\n      labs(colour = \"NEON Site ID\")+\n      theme(legend.position = \"right\",\n            axis.title = element_text(size = 16))\n  })\n\n  output$result &lt;- renderText({\n    paste(\"You have selected \", species$scientificName[selection()])\n  })\n}\n\nUser Interface Function\n\n\n\nui = fluidPage(\n  theme = shinytheme(\"readable\"),\n  h3(\"NEON Woody Plant Vegetation Structure Data\"),\n  p(\"This app allows for quick visualization of tree height and diameter data for the 20 most common species in the NEON woody vegetation structure dataset. The data have been filtered to remove dead trees and obvious outliers for height and diameter. \"),\n  HTML(\"&lt;p style='color:#808080'&gt;NEON (National Ecological Observatory Network). Woody plant vegetation structure, RELEASE-2021 (DP1.10098.001). https://doi.org/10.48443/e3qn-xw47. Dataset accessed from https://data.neonscience.org on November 24, 2021 &lt;/p&gt;\"),\n  p(\"\\n\"),\n  fluidRow(column(width = 12, selectInput(inputId = \"optionNum\",\n                                          label = \"Choose a Species Group\",\n                                          choices = input_options,\n                                          selected = 1))),\n  fluidRow(column(width = 1, checkboxInput(\"Live\", \"Live\", TRUE)),\n           column(width = 2, checkboxInput(\"Live_DD\", \"Live, disease damaged\", TRUE,)),\n           column(width = 2, checkboxInput(\"Live_PD\", \"Live, physically damaged\", TRUE)),\n           column(width = 2, checkboxInput(\"Live_BB\", \"Live, broken bole\", TRUE)),\n           column(width = 2, checkboxInput(\"Live_ID\", \"Live, insect damaged\", TRUE)),\n           column(width = 2, checkboxInput(\"Live_OD\", \"Live,  other damage\", TRUE))),\n\n  mainPanel(tabsetPanel(type = \"tabs\",\n                        tabPanel(title =\"Plot\",  plotOutput('plot1')),\n                        tabPanel(\"Data Table\",  DT::dataTableOutput('table')))\n  )\n)\n\nApp Setup\n\n\n# load saved datasets\nveg = readRDS('./veg_data_for_shiny.Rdata')\nspecies = readRDS('./species.Rdata')\n\n# format input options\ninput_options = species$num\nnames(input_options) = species$scientificName\n\nRun the App\n\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "portfolio/HarvardForest.html",
    "href": "portfolio/HarvardForest.html",
    "title": "Studying Tree Competition and Mortality",
    "section": "",
    "text": "The goal of this project was to explore the relationship between tree growth, competition, and background (unexplained) mortality. This is a project that I conducted as part of a summer internship at Harvard Forest.\nIn this project, I used both long-term datasets and field-data that I collected. For each 10m diameter forest inventory plot, I matched multiple years of data to determine the annual diameter growth increment for each tree. I also calculated a competition value for each tree in each year. The competition index was based on (1) the ratio of a tree’s size to it’s neighbors and (2) the distance to neighboring trees. I also used time-series analysis to gap-fill values for neighboring trees that did not have data in every year because they were located outside of the plot.\n\n\n\n\n\n\n\n\n\n\n\nGrowth vs Competition Index and Tree Mortality for Three Common Species.\n\n\n\nFor hemlock, a shade-tolerant species, there was almost no correlation between competition index and growth increment. Additionally, there was no mortality recorded for hemlock trees in this time period, even at high levels of competition.\nFor red maple, a fairly shade tolerant species, there was an inverse relationship between competition index and growth. However, competition was not an important predictor of mortality because the mortality occurred evenly across the range of competition values.\nFor red oak, a shade intolerant species, there was a stronger relationship between competition index and growth. Additionally, mortality occurred only at high competition values suggesting that high tree competition may increase likelihood of mortality for red oak."
  },
  {
    "objectID": "portfolio/USGS.html",
    "href": "portfolio/USGS.html",
    "title": "Interactive USGS Stream Temperature Data App",
    "section": "",
    "text": "! []\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this project, I displayed stream temperatures from USGS stations within the Pacific Northwest in an interactive app.\n\n\nI chose to display these data because a major heatwave struck the region in June 2021. Stream temperatures in many locations grew warmer than 68F - a critical threshold over which salmon experience great stress and are more likely to contract diseases or die. Heat-related salmon mortality was recorded in the region as a result of the 2015 and 2021 heat waves.\n\n\n\nInteractive App link\n\n\nCode Repository link\n\n\n\nMy code for this project is shown in a reproducible example below. \n\n\nLaura Puckett\n\n\n11/29/2021\n\n\nR version: 4.1.2\n\n\nPart 1. Download Data\n\n\nLoad Libraries\n\n\nlibrary(dataRetrieval); library(dplyr); library(shinyr);\nlibrary(plotly); library(leaflet)\n\nDefine Downloading Functions\n\n\n# function to download site metadata by state\ndl_site_data = function(state){\n  print(state)\n  sites &lt;- whatNWISdata(stateCd = state, parameterCd=\"00010\", siteType=\"ST\")\n  saveRDS(sites, paste('./Rdata/sites', state, sep = '_'))\n  return(sites)\n}\n\n# function to download stream temperature data by state\ndl_temp_data = function(state, startDate, endDate){\n  print(paste(state, startDate, endDate))\n  streamtemp &lt;- readNWISdata(stateCd = state, service=\"dv\", startDate = startDate,\n                             endDate = endDate,parameterCd=\"00010\", siteType=\"ST\")\n  streamtemp = streamtemp %&gt;%\n    dplyr::select(agency_cd, site_no, dateTime, X_00010_00003, tz_cd)\n  saveRDS(streamtemp, paste('./Rdata/streamtemp', startDate, endDate, state, sep = '_'))\n\n  return(streamtemp)\n}\n\nDownload USGS Data\n\n\nstartDate = '2000-01-01'\nendDate = '2021-11-01'\nstates = c(\"OR\",\"WA\",\"ID\")\n\n# Download Site Metadata\nfor(state in states){\n  # download data or read existing file\n  if(file.exists(paste('./Rdata/sites', state, sep = '_'))){\n    state_sites = readRDS(paste('./Rdata/sites',  state, sep = '_'))\n  }else{\n    state_sites = dl_site_data(state)\n  }\n  # append data from multiple states into single dataframe\n  if(state == states[1]){\n    site_data = state_sites\n  }else{\n    site_data = rbind(site_data, state_sites)\n    saveRDS(site_data, './Rdata/site_data.rds')\n  }\n}\n\n# Download Stream Data\nfor(state in states){\n  # download data or read existing file\n  if(file.exists(paste('./Rdata/streamtemp', startDate, endDate, state, sep = '_'))){\n    state_data = readRDS(paste('./Rdata/streamtemp', startDate, endDate, state, sep = '_'))\n  }else{\n    state_data = dl_temp_data(state, startDate, endDate)\n  }\n  # append data from multiple states into single dataframe\n  if(state == states[1]){\n    temp_data = state_data\n  }else{\n    temp_data = rbind(temp_data, state_data)\n    saveRDS(temp_data, './Rdata/streamtemp_data.rds')\n  }\n}\n\nPart 2. Organize Data\n\n\nView Site Metadata\n\n\nsite_data = readRDS('./data_prep/Rdata/site_data.rds')\nsite_data %&gt;% head()\n##     agency_cd  site_no                                station_nm site_tp_cd\n## 32       USGS 10382000       ABERT LAKE NEAR VALLEY FALLS, OREG.         ST\n## 66       USGS 10387100          CHEWAUCAN R NR VALLEY FALLS OREG         ST\n## 108      USGS 10392500            SILVIES RIVER NEAR SILVIES, OR         ST\n## 158      USGS 10395000    EAST FORK SILVIES RIVER NEAR LAWEN, OR         ST\n## 203      USGS 10395500  WEST FORK SILVIES RIVER NEAR LAWEN,OREG.         ST\n## 237      USGS 10396000 DONNER UND BLITZEN RIVER NR FRENCHGLEN OR         ST\n##     dec_lat_va dec_long_va coord_acy_cd dec_coord_datum_cd  alt_va alt_acy_va\n## 32    42.60349   -120.1869            U              NAD83 4247.78        0.1\n## 66    42.51571   -120.2519            U              NAD83      NA         NA\n## 108   43.92269   -118.9583            S              NAD83 4530.00       20.0\n## 158   43.42636   -118.8022            5              NAD83 4104.00        4.3\n## 203   43.38321   -118.8344            U              NAD83 4090.00       20.0\n## 237   42.79083   -118.8675            S              NAD83 4254.00       20.0\n##     alt_datum_cd   huc_cd data_type_cd parm_cd stat_cd  ts_id loc_web_ds\n## 32        NGVD29 17120006           qw   00010    &lt;NA&gt;      0       &lt;NA&gt;\n## 66          &lt;NA&gt; 17120006           qw   00010    &lt;NA&gt;      0       &lt;NA&gt;\n## 108       NAVD88 17120002           qw   00010    &lt;NA&gt;      0       &lt;NA&gt;\n## 158       NAVD88 17120002           qw   00010    &lt;NA&gt;      0       &lt;NA&gt;\n## 203       NGVD29 17120001           qw   00010    &lt;NA&gt;      0       &lt;NA&gt;\n## 237       NGVD29 17120003           dv   00010   00001 113057       &lt;NA&gt;\n##     medium_grp_cd parm_grp_cd  srs_id access_cd begin_date   end_date count_nu\n## 32            wat         PHY 1645597         0 1972-09-29 1977-09-13        3\n## 66            wat         PHY 1645597         0 1969-01-03 1971-09-15       18\n## 108           wat         PHY 1645597         0 1980-06-20 1980-06-20        1\n## 158           wat         PHY 1645597         0 1973-02-09 2017-07-19        2\n## 203           wat         PHY 1645597         0 2017-07-19 2017-07-19        1\n## 237           wat        &lt;NA&gt; 1645597         0 2010-10-14 2021-11-20     3889\n\nView Stream Temperature Data\n\n\nstream_temp = readRDS('./data_prep/Rdata/streamtemp_data.rds')\nstream_temp %&gt;% head()\n##     agency_cd  site_no   dateTime X_00010_00003 tz_cd\n## 117      USGS 10396000 2010-10-14           9.8   UTC\n## 118      USGS 10396000 2010-10-15          11.0   UTC\n## 119      USGS 10396000 2010-10-16          11.2   UTC\n## 120      USGS 10396000 2010-10-17          10.7   UTC\n## 121      USGS 10396000 2010-10-18           9.3   UTC\n## 122      USGS 10396000 2010-10-19           8.2   UTC\n\nReformat Temperature Data\n\n\nstream_temp = stream_temp %&gt;%\n  # convert Celsius to Fahrenheit\n  rename(tempC = X_00010_00003) %&gt;%\n  mutate(tempF = tempC*(9/5)+32,\n         year = lubridate::year(dateTime),\n         month = lubridate::month(dateTime),\n         month_name = lubridate::month(dateTime, label = T),\n         day = lubridate::day(dateTime),\n         # reformat dates for plot axes\n         month_day = paste0(month_name, day, sep = \"-\"),\n         axis_date_minimal = ifelse(day %in% c(1,15), month_day,\"\"),\n         date_all_2021 = dateTime)\n# create a dummy column with 2021 as the year for all to easily\n# plot multiple years of data on top of each other\nlubridate::year(stream_temp$date_all_2021) = 2021\n\nFiltering\n\n\n# get site_no that have sufficient 2021 data for plotting\nsites_with_enough_2021_data = stream_temp %&gt;%\n  filter(year == 2021) %&gt;%\n  group_by(site_no) %&gt;%\n  dplyr::summarize(count = n()) %&gt;%\n  filter(count &gt; 300) %&gt;%\n  dplyr::select(site_no)\n\n# filter sites dataset\nsites_sub = site_data %&gt;%\n  filter(site_no %in% sites_with_enough_2021_data$site_no) %&gt;%\n  mutate(huc_cd = as.numeric(huc_cd)) %&gt;%\n  filter(end_date&gt;= \"2021-08-01\" & begin_date &lt;= \"2010-04-01\") %&gt;%\n  filter(count_nu &gt; 1000) %&gt;% # total number of records\n  filter(huc_cd &gt;= 17000000 & huc_cd &lt; 18000000) # Colombia River Basin hucs\n\n# filter stream temp dataset\nplotData = stream_temp %&gt;%\n  filter(site_no %in% sites_sub$site_no) %&gt;%\n  filter(tempF &lt; 140) # remove really high outlier data for one site\n\n# select columns of interest in both datasets\nplotData = plotData %&gt;%\n  dplyr::select(site_no, dateTime, tempF, year, month,\n                month_day, axis_date_minimal, date_all_2021)\n\nsitesData = sites_sub %&gt;%\n  dplyr::select(site_no, station_nm, dec_lat_va, dec_long_va)\n\n# Filter for the data that will be used in each figure on the app\ndata_for_plot1 = plotData %&gt;%\n  filter(dateTime &lt; '2021-09-30' & dateTime &gt; '2021-04-01') %&gt;%\n  inner_join(sitesData %&gt;% dplyr::select(site_no, station_nm)) %&gt;%\n  unique() %&gt;%\n  group_by(site_no)\n\ndata_for_plot2 = plotData %&gt;%\n  filter(month &gt;= 4 & month &lt;= 9) %&gt;%\n  mutate(color_group = \"other years\",\n         color_group = ifelse(year == 2021, 2021, color_group),\n         color_group = ifelse(year == 2015, 2015, color_group)) %&gt;%\n  inner_join(sitesData %&gt;% dplyr::select(site_no, station_nm)) %&gt;%\n  unique() %&gt;%\n  group_by(year)\n\nWrite Output Files for Shiny App\n\n\nsaveRDS(plotData, './data_prep/Rdata/plotData.rds')\nsaveRDS(sitesData, './shiny_app/sitesData.rds')\nsaveRDS(data_for_plot1, './shiny_app/data_for_plot1')\nsaveRDS(data_for_plot2, './shiny_app/data_for_plot2')\n\nPart 3. The Shiny app\n\n\nServer Function\n\n\nserver &lt;- function(input, output) {\n  # Generate Interactive Map\n  output$map &lt;- renderLeaflet(\n    map &lt;- leaflet() %&gt;%\n      setView(lng = -117.64, lat = 45.88, zoom = 5.4) %&gt;%\n      addProviderTiles(providers$Stamen.TerrainBackground, options = providerTileOptions(opacity = 0.5)) %&gt;%\n      addPolygons(data = states_data, fillColor = topo.colors(10, alpha = NULL),\n                  stroke = FALSE) %&gt;%\n      addMarkers(data = sitesData,\n                 ~dec_long_va,\n                 ~dec_lat_va,\n                 label = ~htmltools::htmlEscape(station_nm),\n                 layerId = ~site_no))\n\n  observeEvent(input$map_marker_click, {\n    # save choice from user input in reactive variable\n    choice &lt;- input$map_marker_click[1]\n  }\n  )\n\n  output$plot1 &lt;- renderPlotly(\n    plot1 &lt;- p_1 %&gt;%\n      add_trace(data = data_for_plot1 %&gt;%\n                  filter(site_no == ifelse(\n                    is.null(input$map_marker_click[1]),\n                    14169000,\n                    input$map_marker_click[1])),\n                type = \"scatter\",\n                x = ~dateTime,\n                y = ~tempF,\n                mode = 'lines',\n                name = \"Selected Site\",\n                line = list(width=2.5, color=\"black\"),\n                opacity = 1,\n                showlegend = T)\n  )\n\n  # Generate Plot 2\n  output$plot2 &lt;- renderPlotly(\n    plot2 &lt;- plot_ly() %&gt;%\n      layout(title = \"\",\n             yaxis=list(title='Stream Temperature [F]'),\n             xaxis = list(title = \"\",\n                          type = 'date',\n                          tickformat = \" %B\")) %&gt;%\n      add_trace(data = data_for_plot2 %&gt;%\n                  filter(year == 2021)  %&gt;%\n                  filter(site_no == ifelse(is.null(choice),14169000,choice)),\n                name = \"2021\",\n                type = \"scatter\",\n                x = ~date_all_2021,\n                y = ~tempF,\n                mode = 'lines',\n                line = list(width=2.5, color=\"black\",\n                            size = 2.5)) %&gt;%\n      add_trace(data = data_for_plot2 %&gt;%\n                  filter(year == 2015)  %&gt;%\n                  filter(site_no == ifelse(is.null(choice),14169000,choice)),\n                type = \"scatter\",\n                x = ~date_all_2021,\n                name = \"2015\",s\n                y = ~tempF,\n                mode = 'lines',\n                line = list(width=2.5, color=\"gray\"),\n                size = 2.5) %&gt;%\n      add_trace(data = data_for_plot2 %&gt;%\n                  filter(color_group == \"other years\")  %&gt;%\n                  filter(site_no == ifelse(is.null(choice),14169000,choice)),\n                type = \"scatter\",\n                x = ~date_all_2021,\n                y = ~tempF,\n                mode = 'lines',\n                yaxis=\"y1\",\n                name = \"other years\",\n                color =~color_group,\n                line=list(color='#1f77b4'),\n                opacity=0.4,\n                showlegend = T)\n  )\n}\n\nUser Interface Function\n\n\nui &lt;- fluidPage(\n  headerPanel('Stream Temperature in the Pacific Northwest'),\n  mainPanel(h3(\"Choose a USGS station from the map. \"),\n            leafletOutput(outputId = \"map\"),\n            h3(\"\"),\n            tabsetPanel(type = \"tabs\",\n                        tabPanel(\"All Sites, 2021\",  plotlyOutput('plot1')),\n                        tabPanel(\"Selected Site, 2000-2021\",  plotlyOutput('plot2'))\n            )\n  )\n)\n\nApp Setup\n\n\nheatwave_start = '2021-06-10'\nheatwave_end =  '2021-07-20'\n\ndata_for_plot1 = readRDS('./shiny_app/data_for_plot1')\ndata_for_plot2 = readRDS('./shiny_app/data_for_plot2')\nsitesData = readRDS('./shiny_app/sitesData.rds')\nstates_data = maps::map('state', regions=c('oregon', 'washington','idaho'), fill = T, plot = F)\n\n# Initialize plot 1 here (and not in server function) because it only needs to be run once.\np_1 &lt;- plot_ly() %&gt;%\n  layout(# title = \"Stream Temperature for All Sites in 2021\",\n    yaxis=list(title='Stream Temperature [F]'),\n    xaxis = list(title = '',\n                 type = 'date',\n                 tickformat = \" %B&lt;br&gt;%Y\"),\n    shapes = list(list(type = \"rect\",\n                       fillcolor = \"red\", line = list(color = \"red\"), opacity = 0.2,\n                       y0 = 35, y1 = 85, x0 = heatwave_start, x1 = heatwave_end),\n                  list(type = \"line\", x0 = \"2021-04-01\", x1 = '2021-09-30',\n                       y0 = 68, y1 = 68,\n                       line = list(color = \"black\", dash = \"dash\")))) %&gt;%\n  add_text(x = c(\"2021-07-01\", \"2021-05-02\"), y = c(38, 71),\n           text = c(\"2021 Heatwave\",\"Dangerous Temperature \\n for Salmon\"),\n           showlegend = F) %&gt;%\n  add_trace(data = data_for_plot1,\n            type = \"scatter\",\n            x = ~dateTime,\n            y = ~tempF,\n            mode = 'lines',\n            name = ~site_no,\n            line=list(width=0.8),\n            opacity=.5,\n            showlegend = F) \n\nRun the App\n\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "pages/education.html",
    "href": "pages/education.html",
    "title": "Education",
    "section": "",
    "text": "Emphasis: Ecological and Environmental Informatics\nGPA: 3.92/4.0"
  },
  {
    "objectID": "pages/education.html#m.s.-in-informatics-computing-and-cyber-systems",
    "href": "pages/education.html#m.s.-in-informatics-computing-and-cyber-systems",
    "title": "Education",
    "section": "",
    "text": "Emphasis: Ecological and Environmental Informatics\nGPA: 3.92/4.0"
  },
  {
    "objectID": "pages/education.html#b.s.-in-forest-resources-and-environmental-conservation",
    "href": "pages/education.html#b.s.-in-forest-resources-and-environmental-conservation",
    "title": "Education",
    "section": "B.S. in Forest Resources and Environmental Conservation",
    "text": "B.S. in Forest Resources and Environmental Conservation\n\nVirginia Tech\nMajor: Environmental Informatic Minors: Forestry, Green Engineering GPA: 3.77/4.0\n\n\nSelection of Coursework:\n\nGIS and Remote Sensing\n\nAlgorithms in GIS\nInformation Technology for Natural Resource Management\nApplied Remote Sensing\nAdvanced Survey of Ecoinformatics Tools\nForest Photogrammetry\n\n\n\nComputing\n\nIntro to Programming in Python\nSoftware Development Methodologies\nLarge-scale Data Structures and Algorithms\nData Mining and Machine Learning\nMachine Learning in Ecology\n\n\n\nNatural Resources\n\nLandscape Measurement and Modeling\nFreshwater Ecology\nForest Biology and Dendrology\nEcosystem Dynamics\nApplied Ecological Data Analysis\n\n\n\nScience\n\nApplied Mathematical Modeling\nStatistical Methods for Engineers\nClimate Science\nBiology I/II\nGeneral Chemistry I/II\nPrinciples of Organic Chemistry\n\nGraduate classes are shown in bold"
  }
]